ðŸ›¡ï¸ SafeScanAI

**SafeScanAI** is an AI-powered tool that detects toxic or harmful content (hate speech, bullying, threats, etc.) using a pre-trained NLP model. It helps platforms moderate conversations at scale and maintain safer online communities.

---

ðŸ’¡ Features

- ðŸ” Real-time detection of toxic content  
- âš™ï¸ Uses pre-trained BERT-based model from HuggingFace  
- âœ… Labels text as TOXIC or NON-TOXIC  
- ðŸ“ˆ Displays prediction confidence score  
- ðŸ’» Runs in Google Colab and locally via Streamlit  
- ðŸ“¦ Lightweight, fast, and easy to use  

---

ðŸš€ Link
https://colab.research.google.com/drive/1MniKOqeXlO9C3F7z1rQSjNVWPvYBfEoK?usp=sharing



---
 ðŸ— Architecture Diagram

User Input --> Preprocessing --> ToxicBERT Model --> Output Prediction


---

ðŸ”„ Process Flow

1. User inputs text  
2. Text is preprocessed (cleaning, tokenization)  
3. Passed to `unitary/toxic-bert` model  
4. Model returns a toxicity probability  
5. System labels the input based on a threshold (e.g., 0.5)  
6. Result shown to user  

---

 ðŸ›  Technologies Used

- Python
- HuggingFace Transformers (`unitary/toxic-bert`)
- Google Colab

---

 ðŸ“¸ Screenshots

![image](https://github.com/user-attachments/assets/36b4dc94-7f47-44cf-af6c-17c8927191bf)

---

