ğŸ›¡ï¸ SafeScanAI

**SafeScanAI** is an AI-powered tool that detects toxic or harmful content (hate speech, bullying, threats, etc.) using a pre-trained NLP model. It helps platforms moderate conversations at scale and maintain safer online communities.

---

ğŸ’¡ Features

- ğŸ” Real-time detection of toxic content  
- âš™ï¸ Uses pre-trained BERT-based model from HuggingFace  
- âœ… Labels text as TOXIC or NON-TOXIC  
- ğŸ“ˆ Displays prediction confidence score  
- ğŸ’» Runs in Google Colab and locally via Streamlit  
- ğŸ“¦ Lightweight, fast, and easy to use  

---

ğŸš€ Link
https://colab.research.google.com/drive/1MniKOqeXlO9C3F7z1rQSjNVWPvYBfEoK?usp=sharing



---
 ğŸ— Architecture Diagram

User Input --> Preprocessing --> ToxicBERT Model --> Output Prediction


---

ğŸ”„ Process Flow

1. User inputs text  
2. Text is preprocessed (cleaning, tokenization)  
3. Passed to `unitary/toxic-bert` model  
4. Model returns a toxicity probability  
5. System labels the input based on a threshold (e.g., 0.5)  
6. Result shown to user  

---

 ğŸ›  Technologies Used

- Python
- HuggingFace Transformers (`unitary/toxic-bert`)
- Google Colab

---
ğŸ”® Future Development

- ğŸŒ Multilingual toxic content detection  
- ğŸ™ï¸ Voice-to-text abuse detection  
- ğŸ“Š Admin dashboard for real-time analytics  
- ğŸ‘¥ Crowd feedback & false-positive reporting system  

---

 ğŸ“¸ Screenshots

![image](https://github.com/user-attachments/assets/36b4dc94-7f47-44cf-af6c-17c8927191bf)

---

